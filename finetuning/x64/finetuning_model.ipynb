{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WKzz-bhZ8adm"
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U peft\n",
    "!pip install -q -U trl\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets\n",
    "!pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "os.environ[\"HF_TOKEN\"] = userdata.get('HF_TOKEN')"
   ],
   "metadata": {
    "id": "15PuFkLO801u"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "#set the quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = \"google/gemma-2-2b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)"
   ],
   "metadata": {
    "id": "zNEXmMnn9EtE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "dataset_url = 'LinhDuong/chatdoctor-5k'\n",
    "dataset = load_dataset(dataset_url)\n",
    "dataset"
   ],
   "metadata": {
    "id": "4CJcVyDy_FIz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "class GemmaPrompt:\n",
    "  def __init__(self):\n",
    "    self.user_instrs = []\n",
    "    self.output_instrs = []\n",
    "  def add_user_instr(self, user_instr='', inputs=''):\n",
    "    self.user_instrs.append(f'<start_of_turn>user {user_instr}. Patient: {inputs}<end_of_turn>')\n",
    "  def add_output_instr(self, output_instr):\n",
    "    self.output_instrs.append(f'<start_of_turn>model {output_instr}<end_of_turn>')\n",
    "  def __str__(self):\n",
    "    return \"\".join(self.user_instrs) + '\\n' + \"\".join(self.output_instrs)"
   ],
   "metadata": {
    "id": "3_VUrH4SKPzL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_prompt(data_point):\n",
    "  prompt = GemmaPrompt()\n",
    "  prompt.add_user_instr(user_instr=data_point['instruction'], inputs=data_point['input'])\n",
    "  prompt.add_output_instr(data_point['output'])\n",
    "  return str(prompt)"
   ],
   "metadata": {
    "id": "RHBjaQivEtTl"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "text_column = [generate_prompt(data_point) for data_point in dataset[\"train\"]]\n",
    "new_dataset = dataset[\"train\"].add_column(\"prompt\", text_column)\n",
    "new_dataset"
   ],
   "metadata": {
    "id": "R0t5bv2TNyq8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "new_dataset['prompt'][0]"
   ],
   "metadata": {
    "id": "x2q65fYxzRvp"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = new_dataset.shuffle(seed=1234)  # Shuffle dataset here\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
   ],
   "metadata": {
    "id": "o2bDg5g8OUVF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ],
   "metadata": {
    "id": "4OmtBYurOlAP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "[(dataset['train'][i]['input'], dataset['train'][i]['output']) for i in range(3)]"
   ],
   "metadata": {
    "id": "n07fTzrahIdZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import bitsandbytes as bnb\n",
    "#  Find all the linear layers of the model that could potentially be optimized in \n",
    "def find_all_linear_names(model):\n",
    "  cls = bnb.nn.Linear4bit \n",
    "  lora_module_names = set()\n",
    "  for name, module in model.named_modules():\n",
    "    if isinstance(module, cls):\n",
    "      names = name.split('.')\n",
    "      lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    # needed for 16-bit\n",
    "    if 'lm_head' in lora_module_names:\n",
    "      lora_module_names.remove('lm_head')\n",
    "  return list(lora_module_names)\n",
    "#\n",
    "modules = find_all_linear_names(model)"
   ],
   "metadata": {
    "id": "N_f-AZQVPi1E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "# Gradient Checkpointing will slow the training process for lower memory allocation\n",
    "model.gradient_checkpointing_enable()\n",
    "# preprocess quantized model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ],
   "metadata": {
    "id": "F2R3gRtZO_0K"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "id": "miZEXsp0Pl0z"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import transformers\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side='right'\n",
    "\n",
    "training_args = transformers.TrainingArguments(per_device_train_batch_size=1, gradient_accumulation_steps=4,\n",
    "                                               gradient_checkpointing=True, max_steps=100, learning_rate=2e-4,\n",
    "                                               logging_steps=100, output_dir=\"outputs\", optim=\"paged_adamw_32bit\",\n",
    "                                               save_strategy=\"epoch\", num_train_epochs=1, lr_scheduler_type=\"cosine\", )\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    peft_config=lora_config,\n",
    "    max_seq_length=2500,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "trainer.train()"
   ]
  },
  {
   "metadata": {
    "id": "H-tqGMRF4Ka-"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_completion(query: str, model, tokenizer) -> str:\n",
    "  device = \"cuda:0\"\n",
    "\n",
    "  prompt_template = \"\"\"\n",
    "  <start_of_turn>user\n",
    "  You are a doctor, please answer the medical questions based on the patient's description.\n",
    "  {query}\n",
    "  <end_of_turn>\\n<start_of_turn>model\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  prompt = prompt_template.format(query=query)\n",
    "\n",
    "  encodeds = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "\n",
    "  model_inputs = encodeds.to(device)\n",
    "\n",
    "\n",
    "  generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True, pad_token_id=tokenizer.eos_token_id)\n",
    "  # decoded = tokenizer.batch_decode(generated_ids)\n",
    "  decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "  return (decoded)\n",
    "\n"
   ]
  },
  {
   "metadata": {
    "id": "xZdXwt3JXLxn"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "metadata": {
    "id": "osTSZddoUWc9"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Your existing code for loading and merging the model\n",
    "new_model = \"gemma-2-2b-chatdoctor\"\n",
    "\n",
    "# save model\n",
    "trainer.model.save_pretrained(new_model)\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "merged_model= PeftModel.from_pretrained(base_model, new_model)\n",
    "merged_model= merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "#save_adapter=True, save_config=True\n",
    "merged_model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "metadata": {
    "id": "D75NWzGCZprU"
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "merged_model.push_to_hub(new_model, use_temp_dir=False)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "query = \"\"\" Doctor, I have been experiencing some symptoms associated with Von Hippel-Lindau disease. \"\"\"\n",
    "result = get_completion(query=query, model=merged_model, tokenizer=tokenizer)\n",
    "print(result)"
   ],
   "metadata": {
    "id": "7o0abCJ34mXq"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "OwT8SWCyfIb1"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
